{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we prepare Yelp data to answer business questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from shapely.geometry import Point, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "## Goal\n",
    "\n",
    "In this case, we will introduce you to another uncleaned dataset for practice. We will focus on the nuances of cleaning the dataset parameter by parameter. Being able to dive into a single parameter and investigate its attributes will be crucial for modeling in later stages of the data science proceess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Context.** [Yelp](https://www.yelp.com) is a very popular website, where anyone can write a review about restaurants, hotels, spas or any business. They have decided to start analysing the data and will use the results of the analysis to make decisions about new features to the service. They have now approached you, an independant data consultant with a requirement of cleaning the data they have and for you to help them get more context on the data.\n",
    "\n",
    "**Business Problem.** Your task is to go through the data, and make transformations or additions to the data based on their needs, which are listed below.\n",
    "\n",
    "**Analytical Context.** The client has shared three files with you containing details about the businesses and end-users on the service along with the reviews posted by these users. With this data, they would like you to do the following:\n",
    "\n",
    "1. For each business, find out the county and state that it is a part of\n",
    "2. For each business, calculate the total number of reviews received and the average star rating across those reviews\n",
    "3. For each business, calculate the total number of check-ins by hour of day, by day of week, and across all time\n",
    "    \n",
    "We will run these operations locally with a subset of the data. (As in the previous case, in enterprise-grade production one would use a cloud service provider to apply our cleaning script to the entire dataset, but the data file here is quite large so we will only work with a subset of it on our local machines. You are welcome to read more online about these providers if you wish.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "## Reading in the data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in all three data files provided and familiarize ourselves with the data. Note that we truncate our read to the first 1000 rows due to the sizes of the files. This means we may have some strange results, such as restaurants having few, if any reviews. This is a great opportunity to practice your data skills! Try reading in larger amounts of data, or only reading reviews for the 1000 businesses you read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = pd.read_csv('https://storage.googleapis.com/training-cases-large-case-asset-files/case.data_cleaning_practice/businesses.csv', nrows=1000)\n",
    "reviews = pd.read_csv('https://storage.googleapis.com/training-cases-large-case-asset-files/case.data_cleaning_practice/reviews.csv', nrows=1000)\n",
    "checkins = pd.read_csv('https://storage.googleapis.com/training-cases-large-case-asset-files/case.data_cleaning_practice/checkins.csv', nrows=1000)\n",
    "checkins = checkins.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns present in each of the files are:\n",
    "\n",
    "1. **reviews**\n",
    "    * **review_id** - Unique identifier for the review\n",
    "    * **business_id** - Unique identifier of the business that is being reviewed\n",
    "    * **user_id** - Unique identifier of the user who posted the review\n",
    "    * **date** - The date and time of the review\n",
    "    * **star** - Star rating for the review\n",
    "    * **text** - Review text\n",
    "    * **cool** - Number of cool votes received for the review\n",
    "    * **funny** - Number of funny votes received for the review\n",
    "    * **useful** - Number of funny votes received for the review\n",
    "    \n",
    "    \n",
    "2. **businesses**\n",
    "    * **business_id** - Unique identifier of the business\n",
    "    * **name** - Name of the business\n",
    "    * **categories** - Categories associated with the business\n",
    "    * **latitude** - Location of the business (latitude)\n",
    "    * **longitude** - Location of the business (longitude)\n",
    "    * **review_count** - Number of reviews received for the business\n",
    "    * **stars** - Average star rating, rounded to half stars\n",
    "    \n",
    "    \n",
    "3. **checkins**\n",
    "    * **business_id** - Unique identifier of the business\n",
    "    * **date** - List of datetimes when users checked in to the business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "## Handling null values in `reviews`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, handling null values is a staple of cleaning datasets. Let's uncover the columns in the table `reviews` that contain null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "cool            True\n",
       "date            True\n",
       "funny           True\n",
       "review_id      False\n",
       "stars           True\n",
       "text           False\n",
       "useful          True\n",
       "user_id        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that null values are present in the following columns: `date`, `stars`, `cool`, `funny`, and `useful`. Each of these columns need to be handled in a different way.\n",
    "\n",
    "In contrast to the previous case, here we will look at some advanced methods for handling null values that don't use normal `pandas` operations. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "## `date`\n",
    "\n",
    "Since the client has indicated they will likely be using the cleaned data for numerous analyses later on, removing rows with missing values or even writing in meaningless filler values will not work. So we need to figure out a sensible interpolation method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1SWheh84yJXfytovILXOAQ</td>\n",
       "      <td>Golf, Active Life</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>33.522143</td>\n",
       "      <td>-112.018481</td>\n",
       "      <td>Arizona Biltmore Golf Club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
       "      <td>Specialty Food, Restaurants, Dim Sum, Imported...</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>43.605499</td>\n",
       "      <td>-79.652289</td>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
       "      <td>Sushi Bars, Restaurants, Japanese</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xvX2CttrVhyG2z1dFg_0xw</td>\n",
       "      <td>Insurance, Financial Services</td>\n",
       "      <td>Goodyear</td>\n",
       "      <td>33.455613</td>\n",
       "      <td>-112.395596</td>\n",
       "      <td>Farmers Insurance - Paul Lorenz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HhyxOkGAM07SRYtlQ4wMFQ</td>\n",
       "      <td>Plumbing, Shopping, Local Services, Home Servi...</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>35.190012</td>\n",
       "      <td>-80.887223</td>\n",
       "      <td>Queen City Plumbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68dUKd8_8liJ7in4aWOSEA</td>\n",
       "      <td>Shipping Centers, Couriers &amp; Delivery Services...</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>43.599475</td>\n",
       "      <td>-79.711584</td>\n",
       "      <td>The UPS Store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5JucpCfHZltJh5r1JabjDg</td>\n",
       "      <td>Beauty &amp; Spas, Hair Salons</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>50.943646</td>\n",
       "      <td>-114.001828</td>\n",
       "      <td>Edgeworxx Studio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gbQN7vr_caG_A1ugSmGhWg</td>\n",
       "      <td>Hair Salons, Hair Stylists, Barbers, Men's Hai...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>36.099872</td>\n",
       "      <td>-115.074574</td>\n",
       "      <td>Supercuts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Y6iyemLX_oylRpnr38vgMA</td>\n",
       "      <td>Nail Salons, Beauty &amp; Spas, Day Spas</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>33.654815</td>\n",
       "      <td>-112.188568</td>\n",
       "      <td>Vita Bella Fine Day Spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4GBVPIYRvzGh4K4TkRQ_rw</td>\n",
       "      <td>Beauty &amp; Spas, Nail Salons, Day Spas, Massage</td>\n",
       "      <td>Fairview Park</td>\n",
       "      <td>41.440825</td>\n",
       "      <td>-81.854097</td>\n",
       "      <td>Options Salon &amp; Spa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                         categories  \\\n",
       "0  1SWheh84yJXfytovILXOAQ                                  Golf, Active Life   \n",
       "1  QXAEGFB4oINsVuTFxEYKFQ  Specialty Food, Restaurants, Dim Sum, Imported...   \n",
       "2  gnKjwL_1w79qoiV3IC_xQQ                  Sushi Bars, Restaurants, Japanese   \n",
       "3  xvX2CttrVhyG2z1dFg_0xw                      Insurance, Financial Services   \n",
       "4  HhyxOkGAM07SRYtlQ4wMFQ  Plumbing, Shopping, Local Services, Home Servi...   \n",
       "5  68dUKd8_8liJ7in4aWOSEA  Shipping Centers, Couriers & Delivery Services...   \n",
       "6  5JucpCfHZltJh5r1JabjDg                         Beauty & Spas, Hair Salons   \n",
       "7  gbQN7vr_caG_A1ugSmGhWg  Hair Salons, Hair Stylists, Barbers, Men's Hai...   \n",
       "8  Y6iyemLX_oylRpnr38vgMA               Nail Salons, Beauty & Spas, Day Spas   \n",
       "9  4GBVPIYRvzGh4K4TkRQ_rw      Beauty & Spas, Nail Salons, Day Spas, Massage   \n",
       "\n",
       "            city   latitude   longitude                             name  \n",
       "0        Phoenix  33.522143 -112.018481       Arizona Biltmore Golf Club  \n",
       "1    Mississauga  43.605499  -79.652289       Emerald Chinese Restaurant  \n",
       "2      Charlotte  35.092564  -80.859132      Musashi Japanese Restaurant  \n",
       "3       Goodyear  33.455613 -112.395596  Farmers Insurance - Paul Lorenz  \n",
       "4      Charlotte  35.190012  -80.887223              Queen City Plumbing  \n",
       "5    Mississauga  43.599475  -79.711584                    The UPS Store  \n",
       "6        Calgary  50.943646 -114.001828                 Edgeworxx Studio  \n",
       "7      Las Vegas  36.099872 -115.074574                        Supercuts  \n",
       "8       Glendale  33.654815 -112.188568          Vita Bella Fine Day Spa  \n",
       "9  Fairview Park  41.440825  -81.854097              Options Salon & Spa  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>na4Th5DrNauOv-c43QQFvA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2004-10-19 02:46:40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>xW294l3Lwh0cxlHU1jwRDA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The gold standard for casinos everywhere. Just...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>nkN_do3fJ9xekchVC-v68A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u8C8pRvaHXg3PgDrsUHJHQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-10-19 19:24:13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0QHCY_55TFHHvyumEMpDew</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good stuff. Pricey by normal pizza standards.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nkN_do3fJ9xekchVC-v68A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EZOoB2D8uQHV_gJoGCMTxQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-10-19 21:33:08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1Iobyi_7BkFON25Oegs0aw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Love their subs. Cheap and top shelf ingredients.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nkN_do3fJ9xekchVC-v68A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oYMsq2Xvzw6UbrIlMWjb-A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-10-19 21:34:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2F5J51OYtD49eyIUKJKVgg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Love their pizza. They used to have a great ta...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nkN_do3fJ9xekchVC-v68A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AtLv64FV-Pw6JuT3XUKU1g</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-10-19 21:35:14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pho1XNCTeRxQVzWR_5vacg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pokey Sticks are the best!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nkN_do3fJ9xekchVC-v68A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ydUqgWsF3F27TbauOyib0w</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2004-12-19 20:47:24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ef1skKLKZ9izwBmreb_-qw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Frequently busy due to their great food, but t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62GNFh5FySkA3MbrQmnqvg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>N2PlDjUJVfOJzsPzY0Au1w</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-12-19 20:56:54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6POnAs_4MijROSKeOevXHQ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Not the best part of town.  Not particularly g...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62GNFh5FySkA3MbrQmnqvg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ikubvyZFO0kxhA56RETzIg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mNNTXbRPA6xKsWAFEJekdA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent compounding pharmacy, affectionately...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23J4vG9_xxxdnmi8CBX7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ITieHQ8UwKq74FnqTKnPOQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2005-03-14 18:50:04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>agDLOa-3a8tA6o_S-4fGfQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The coolest corner store in all of the world. ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>GXn4ZsasLKh0qZ5g3nIqcQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uoUa8ugZLrly0bA268IqEg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2005-03-14 18:52:13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rnWCfxWkA10VP7QKj2z3wg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Oh man, where else can you get a blueberry mil...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GXn4ZsasLKh0qZ5g3nIqcQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  na4Th5DrNauOv-c43QQFvA   5.0  2004-10-19 02:46:40    3.0   \n",
       "1  u8C8pRvaHXg3PgDrsUHJHQ   0.0  2004-10-19 19:24:13    1.0   \n",
       "2  EZOoB2D8uQHV_gJoGCMTxQ   0.0  2004-10-19 21:33:08    0.0   \n",
       "3  oYMsq2Xvzw6UbrIlMWjb-A   0.0  2004-10-19 21:34:40    0.0   \n",
       "4  AtLv64FV-Pw6JuT3XUKU1g   0.0  2004-10-19 21:35:14    0.0   \n",
       "5  ydUqgWsF3F27TbauOyib0w   1.0  2004-12-19 20:47:24    1.0   \n",
       "6  N2PlDjUJVfOJzsPzY0Au1w   0.0  2004-12-19 20:56:54    0.0   \n",
       "7  ikubvyZFO0kxhA56RETzIg   0.0                  NaN    1.0   \n",
       "8  ITieHQ8UwKq74FnqTKnPOQ   1.0  2005-03-14 18:50:04    2.0   \n",
       "9  uoUa8ugZLrly0bA268IqEg   1.0  2005-03-14 18:52:13    0.0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  xW294l3Lwh0cxlHU1jwRDA    5.0   \n",
       "1  0QHCY_55TFHHvyumEMpDew    4.0   \n",
       "2  1Iobyi_7BkFON25Oegs0aw    4.0   \n",
       "3  2F5J51OYtD49eyIUKJKVgg    4.0   \n",
       "4  pho1XNCTeRxQVzWR_5vacg    4.0   \n",
       "5  Ef1skKLKZ9izwBmreb_-qw    4.0   \n",
       "6  6POnAs_4MijROSKeOevXHQ    3.0   \n",
       "7  mNNTXbRPA6xKsWAFEJekdA    5.0   \n",
       "8  agDLOa-3a8tA6o_S-4fGfQ    5.0   \n",
       "9  rnWCfxWkA10VP7QKj2z3wg    5.0   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  The gold standard for casinos everywhere. Just...     5.0   \n",
       "1      Good stuff. Pricey by normal pizza standards.     0.0   \n",
       "2  Love their subs. Cheap and top shelf ingredients.     0.0   \n",
       "3  Love their pizza. They used to have a great ta...     0.0   \n",
       "4                         Pokey Sticks are the best!     0.0   \n",
       "5  Frequently busy due to their great food, but t...     1.0   \n",
       "6  Not the best part of town.  Not particularly g...     0.0   \n",
       "7  Excellent compounding pharmacy, affectionately...     1.0   \n",
       "8  The coolest corner store in all of the world. ...     2.0   \n",
       "9  Oh man, where else can you get a blueberry mil...     0.0   \n",
       "\n",
       "                  user_id  \n",
       "0  nkN_do3fJ9xekchVC-v68A  \n",
       "1  nkN_do3fJ9xekchVC-v68A  \n",
       "2  nkN_do3fJ9xekchVC-v68A  \n",
       "3  nkN_do3fJ9xekchVC-v68A  \n",
       "4  nkN_do3fJ9xekchVC-v68A  \n",
       "5  62GNFh5FySkA3MbrQmnqvg  \n",
       "6  62GNFh5FySkA3MbrQmnqvg  \n",
       "7  23J4vG9_xxxdnmi8CBX7Ng  \n",
       "8  GXn4ZsasLKh0qZ5g3nIqcQ  \n",
       "9  GXn4ZsasLKh0qZ5g3nIqcQ  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "\n",
    "### Exercise 1:\n",
    "\n",
    "Given what we know about the data so far, describe a sensible interpolation method that we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code that implements our idea above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows = reviews[reviews['date'].isnull()] # all rows where I am missing a date\n",
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "for index, row in nan_rows.iterrows():\n",
    "    previous_date = reviews.iloc[index - 1]['date']\n",
    "    if isinstance(previous_date, str): # if the date of the previous row is not missing\n",
    "        previous_date = datetime.datetime.strptime(previous_date, date_format) # let's format it correctly\n",
    "    next_date = None\n",
    "    next_date_count = 1\n",
    "    while next_date is None: # keep trying to look at the next_date if it's still missing\n",
    "        try:\n",
    "            next_date = datetime.datetime.strptime(reviews.iloc[index + next_date_count]['date'], date_format)\n",
    "        except Exception:\n",
    "            next_date_count += 1\n",
    "    difference = (next_date - previous_date).seconds\n",
    "    current_date = previous_date + datetime.timedelta(seconds=difference/(next_date_count + 1))\n",
    "    reviews.loc[index, 'date'] = current_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "10_min"
    ]
   },
   "source": [
    "## `star`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next feature which has null values to handle is `star`. `star` is a numeric value and can range from 1 to 5, where 1 represents the lowest rating and 5 the highest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "10_min"
    ]
   },
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Describe an appropriate method for filling in the missing values of `star`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the `star` rating based on columns that you do observe (cool, funny, useful, text). Sentiment analysis on the `text` feautre in partciular might be useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about interpolation using mean/median `star` value?\n",
    "Pro: simple!\n",
    "Con: introduce (perpetuate) bias, falsely deflate the variance, so be cautious about using this if you might be doing hypothesis testing/feaure importance analysis, all of these things require an accurate understanding of the...\n",
    "\n",
    "Interpolation via model from known values: \n",
    "\n",
    "Impute the `star` rating based on columns that you do observe. (`cool`, `funny`, `useful`, `text`). \n",
    "Sentiment analysis on the `text` feature in particular might be useful!\n",
    "\n",
    "start = NA <-- ?\n",
    "cool = 0\n",
    "funny = 0\n",
    "useful = 1\n",
    "text = \"This wasn't that great of a restaurant because I had to wait in line for 20 min longer that what they told me!\"\n",
    "\n",
    "start = 2\n",
    "cool = 0\n",
    "funny = 0\n",
    "useful = 1\n",
    "text = \"This was annoying because I had to wait so long!\"\n",
    "\n",
    "Y ~ f(x)\n",
    "model #1 (sentiment analysis)\n",
    "`sentiment_of_text` ~ `text`\n",
    "categorical(\"positive\", \"negative\", \"neutral\", \"is_it_missing\")\n",
    "\n",
    "model #2 \n",
    "`star` ~ f(`cool`, `funny`, `useful`, `sentiment_of_text`, `business_id`, `date`)\n",
    "\n",
    "Maybe we want to assume `star` ratings stay quite stable for a given businees day-over-day HOWEVER, they could change a lot year-over-year, and so we could use `star` ratings for a given business on the same day/week/month to interpolate the `star` ratings for reviews where they are missing.\n",
    "\n",
    "data cleaning --> data exploration --> data modeling --> data training --> deploy...\n",
    "\n",
    "What can we understand about *how* a value is missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "10_min"
    ]
   },
   "source": [
    "## `cool`, `funny`, `useful` and revisiting interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a similar logic to conclude that the `cool`, `funny`, and `useful` columns should also be filled with `NaN`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['cool'].fillna(np.nan, inplace=True)\n",
    "reviews['funny'].fillna(np.nan, inplace=True)\n",
    "reviews['useful'].fillna(np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assume, for the sake of progress, that the inputs `cool`, `date`, `funny`, `useful`, and `text` ARE good predictors for `star`, and that the client is mostly interested in high-level properties of the distributions of these variables and how they correlate to each other. In such a case, we can go ahead and leverage the power of machine learning to predict a reasonable estimate for `star`. \n",
    "\n",
    "We will do so by using the [IterativeImputer](https://scikit-learn.org/stable/modules/impute.html#multivariate-feature-imputation) class of `scikit-learn` to fill the missing values in these columns. `IterativeImputer` fills one column after another by building models for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "cool            True\n",
       "date           False\n",
       "funny           True\n",
       "review_id      False\n",
       "stars           True\n",
       "text           False\n",
       "useful          True\n",
       "user_id        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "cool            True\n",
       "date           False\n",
       "funny           True\n",
       "review_id      False\n",
       "stars          False\n",
       "text           False\n",
       "useful          True\n",
       "user_id        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = IterativeImputer() # TODO: research what this does...\n",
    "\n",
    "cols_to_impute = ['stars', 'cool', 'funny', 'useful']\n",
    "\n",
    "imputed_df = pd.DataFrame(imputer.fit_transform(reviews[cols_to_impute])) # fit the transformation and then cast it as a DF\n",
    "imputed_df\n",
    "imputed_df.columns = cols_to_impute\n",
    "\n",
    "reviews[['stars']] = imputed_df['stars'] # could be reviews[cols_to_impute] = imputed_df[cols_to_impute]\n",
    "reviews.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value has to be dynamically filled, similar to the cool and funny columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with erroneous values\n",
    "\n",
    "Of course, missing values are not the only problem in datasets – erroneous values are too! So we should look at the other columns with non-null values to see if they could have errors that we can correct. Here, the other columns are `business_id`, `review_id`, `user_id`, and `text`. However, IDs are arbitrary identifiers which we have no way of ascertaining if they are correct or not, so we have to take the values we are given for granted.\n",
    "\n",
    "`text` is the text of the user review, but the objective of any attempts to clean this is unclear - what metrics would we use to determine if a review text is \"clean\" or not? The answer to this question is not obvious at all without much more precise direction from the client about what they intend to use this text for, so we will leave it alone for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "## Cleaning up `businesses`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by checking which columns contain null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "categories      True\n",
       "city           False\n",
       "latitude       False\n",
       "longitude      False\n",
       "name           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are null values in `categories`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Suppose for a moment that there were missing values in the `city` column. Can you describe a method which would allow us to effectively fill in missing `city` values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for a given row, we ahave `latitude` and `longtitude`, then we can probably impute `city`!\n",
    "\n",
    "`city` = NA\n",
    "`latitude` = 34.19212\n",
    "`longitude` = 20.29382\n",
    " \n",
    " What sort of Lookup table do you need?\n",
    " `city` `latitude` `longitude`\n",
    " \n",
    " And if there's a `city` missing from our Lookup table: (we could still impute by simply looking at the closest `longitude` and `latitude` in our `business` table)\n",
    " \n",
    " \n",
    "How do we define closest?\n",
    "\n",
    "m = city is missing\n",
    "(lat_m, lon_m)\n",
    "\n",
    "a = another row where city is not missing\n",
    "(lat_a, lon_a)\n",
    "\n",
    "Calculate the Euclidian distance!\n",
    "d = sqrt{ (lat_m - lat_a)^2 + (lon_m - lon_a)^2 }\n",
    "\n",
    "and then calculate d for all rows where city is not missing and pick the row where d is smallest! \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Describe and implement the best method to fill in the null values in the `categories` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ideas: \n",
    "1. We can ise if there's information in the business name that suggests the category (e.g. \"Sunset Golf\" is probably related to \"Golf\")\n",
    "3. If the business name is missing we can join the business_id to the `reviews` table use the review's `text` to see if there's a mention of categories (e.g. \"The food tasted great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that this is not an ideal solution as those businesses do have a category associated with them; we just do not know what it is. It could be random that some businesses were not documented with a category but it is also possible that we are dealing with a systematic error here. For example, perhaps all the businesses in a specific area were not documented. Our default solution is the best option we have, but a good data scientist should note when his/her imputation methods are subject to the possibility of high errors and what additional information they would need to address those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another look at erroneous values\n",
    "\n",
    "Of course, missing values are not the only thing that can go wrong in a dataset – erroneous values can affect the analysis as well. Looking at the columns which we have not dealt with yet, we have `business_id`, `latitude`, `longitude`, and `name` as potential candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Describe and implement suitable methods to deal with potential erroneous values in these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if `business_id` is wrong? \n",
    "- The main issue I'd be worried about is the same business somehow getting entered twice i.e. McDonalds vs mcdonalds\n",
    "- How could I check for duplicates?\n",
    "- Use fuzzy match on `longitude`/ `latitude`/`name`  # TODO: reserach 'fuzzy' match!\n",
    "- Ex: `Felix Trust Financial Advisors`, Boise, Idaho --> uncommon, probably a duplicate of in table more than once \n",
    "\n",
    "What if `latitude`, `longtitude` is wrong? \n",
    "- format is not lat/lon (e.g. impossible or location doesn't make sense, e.g. in the middle of the Pacific)\n",
    "- if you have a `city`, you can also check if it matches the `city`\n",
    "- if you know the business only operates in a certain country, then a location outside of that country doesn't make sense\n",
    "\n",
    "What if `name` is wrong? \n",
    "- non-sensical characters ({, ], <, >, %)\n",
    "- random character ('fdskkhsdf', 'sdfl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "## Adding information for `businesses`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the client's request, we must find the county and state each business belongs in and add this to the table. We use the geoJSON files that contain the geoshapes of each state and county in the US. We will be using the [shapely](https://shapely.readthedocs.io/en/latest/manual.html) library to figure out whether a point is present inside the shape that represents the border of each state/county:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'your_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7cfbddc1577a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#generalization:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0myour_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_column_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myour_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myour_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'your_table' is not defined"
     ]
    }
   ],
   "source": [
    "with open('us-state-shapes.json') as f:\n",
    "    states = json.load(f)\n",
    "    \n",
    "def get_state_name(row):\n",
    "    if not row['latitude'] or not row['longitude']:\n",
    "        return None\n",
    "    point = Point(row['longitude'], row['latitude'])\n",
    "    for state in states['features']:\n",
    "        polygon = shape(state['geometry'])\n",
    "        if polygon.contains(point):\n",
    "            return state['properties']['NAME']\n",
    "        \n",
    "businesses['state'] = businesses.apply(get_state_name, axis=1)\n",
    "\n",
    "#generalization:\n",
    "your_table['new_column_name'] = your_table.apply(your_function, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 6:\n",
    "\n",
    "Find the county of each business and add them to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('us-county-shapes.json') as f:\n",
    "    counties = json.load(f)\n",
    "    \n",
    "def get_county_name(row):\n",
    "    if not row['latitude'] or not row['longitude']:\n",
    "        return None\n",
    "    point = Point(row['longitude'], row['latitude'])\n",
    "    for county in counties['features']:\n",
    "        polygon = shape(county['geometry'])\n",
    "        if polygon.contains(point):\n",
    "            return county['properties']['NAME']\n",
    "businesses['county'] = businesses.apply(get_county_name, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I know the name of the geoJSON file I need to grab?\n",
    "How do I know whether `county` objects have `geometry` and `properties` fields?\n",
    "Check the `shapely` documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add information about a restaurant's number of reviews and their average star rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 7:\n",
    "\n",
    "Calculate the number of reviews received for each restaurant. Remember, if you're using a truncated dataset, you may have restaurants with zero reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_review_count(row):\n",
    "    business_id = row[\"business_id\"]\n",
    "    business_reviews = reviews[reviews[\"business_id\"] == business_id]\n",
    "    return len(business_reviews)\n",
    "\n",
    "businesses['review_count'] = businesses.apply(calculate_review_count, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 8:\n",
    "\n",
    "For each restaurant, calculate its average star rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW \n",
    "def calculate_average_star(row):\n",
    "    business_id = row[\"business_id\"]\n",
    "    business_reviews = reviews[reviews[\"business_id\"] == business_id]\n",
    "    return len(business_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "20_min"
    ]
   },
   "source": [
    "## Aggregating the check-ins data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do our final task: use the check-ins data given in the JSON file to compute various statistics on the number of check-ins over various time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "20_min"
    ]
   },
   "source": [
    "### Exercise 9:\n",
    "\n",
    "Compute, for each business, the total number of check-ins by hour of day, by weekday, by day, and across all time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "2_min"
    ]
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "In this case, we practiced our skills in dealing with missing values and also learned about common ways of handling erroneous values. We then creatively leveraged external data in order to engineer additional features based on client requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "8_min"
    ]
   },
   "source": [
    "## Takeaways\n",
    "\n",
    "In this case, you looked more at how to deal with missing data. You learned that although interpolation is a powerful tool, when and how to use it depends highly on the projected use cases of the data. Sometimes, it is better to simply replace a missing value with \"Not found\" or `NaN` so that it is clear to end users to discount it appropriately from subsequent analyses, rather than run the risk of skewing the resultant data distribution or summary statistics. Removing rows is generally okay in 2 cases: when your data is corrupted, or if the data has a marginal impact on what you are trying to do. For example, if you don't need a particular piece of data to do anything, it is easier to leave them as `NaN` instead of inputting them.\n",
    "\n",
    "You also learned a few ways of using existing public data to help interpolate missing values in your current dataset. This is an invaluable skill and often times the best way to deal with missing values is to exercise this resourcefulness. It is important to recognize not all interpolation is made equal. If you have many parameters that are imputed poorly or with great uncertainty, it will be much harder to develop an accurate prediction model later if those parameters are key predictors.\n",
    "\n",
    "Finally, you looked at erroneous values and various common ways these could leak into the data. You also learned that sometimes it is too difficult or impossible to systematically determine the existence of error.\n",
    "    \n",
    "We dove into the technical nuances of data cleaning in this case but it is important to note this dataset is very simple compared to the data you will be working with in real life. It is not too uncommon to deal with datasets with hundreds of parameters where dozens have different types of missing/incorrect data. Additionally, you may encounter datasets where you are unsure what some columns/parameters mean. This dataset came with a nice handy dictionary explaining what information is in each column. \n",
    "    \n",
    "Students are highly encouraged to thoroughly review all the EDA-related cases we have taught up to this point. Mastery of EDA will immensely improve the quality of your data wrangling & cleaning process, which in combination will impact your subsequent modeling process in a positive way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
